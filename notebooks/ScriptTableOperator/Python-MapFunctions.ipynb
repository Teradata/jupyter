{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map functions: map_row() and map_partition()\n",
    "\n",
    "Example use cases:\n",
    "* Group grade processing with map_row().\n",
    "* Simple Micromodeling example: Model training and scoring of housing data. Data are partitioned on the basis of the home style feature, and a different model is trained in the Advanced SQL Engine for each home style. Then, test data are scored with the corresponding models. Both operations are performed on the user end through using map_partition().\n",
    "\n",
    "Example goals:\n",
    "* Using the teradataml DataFrame methods map_row() and map_partition() to apply a Python function to each row or group of rows (partition).\n",
    "\n",
    "Notes:\n",
    "* Map functions use the SCRIPT Table Operator (STO) Database object in the background. Therefore, to use map functions the STO **must be enabled** in your target Advanced SQL Engine, and the Teradata Packages for In-nodes Analytics must be installed on its nodes. Specifically, the required packages for Python are **teradata-python** and **teradata-python-addons**\n",
    "* For the Map functions to operate correctly, the **Python** version and the Python *dill* add-on library on the client must be **the same version** as in the In-nodes installed distribution.\n",
    "* This notebook utilizes several Python packages in addition to **teradataml** which you may need to install on your client.\n",
    "* For this example, no additional data files are needed; this example utilizes teradaml built-in data.\n",
    "\n",
    "Notebook workflow:\n",
    "1. Setup environment.\n",
    "2. Illustrate map_row() examples with a few different ways to use map_row().\n",
    "3. Illustrate map_partition() examples: We use map_partition() for Micromodeling, that is\n",
    "   * in a first example we train a different mode for different partitions in a Database table; then,\n",
    "   * in a second example we score partitions of test data in a Database talbe with the corresponding trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import getpass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from collections import OrderedDict\n",
    "from teradataml import create_context, remove_context, load_example_data, DataFrame\n",
    "from teradataml.dataframe.sql_functions import case\n",
    "from teradatasqlalchemy.types import FLOAT, CLOB\n",
    "from sqlalchemy import func\n",
    "from sqlalchemy.sql import literal_column\n",
    "from base64 import b64encode, b64decode\n",
    "from dill import dumps, loads\n",
    "from numpy import asarray\n",
    "from pandas import isna, concat, read_csv, Series\n",
    "\n",
    "# For formatting the output for better readability (only for demo - not required otherwise)\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a Vantage system to connect to. Specify default database accordingly, or delete the argument.\n",
    "host = input(\"Host: \")\n",
    "username = input(\"Username: \")\n",
    "password = getpass.getpass()\n",
    "# Specify a database name and the database argument, if desired to connect to another than the default.\n",
    "database = \"xxxxx\"\n",
    "con = create_context(host = host, username = username, password = password, database = database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. map_row(): Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The map_row() examples use the 'admissions_train' dataset. In these examples, the average 'gpa'\n",
    "# of each student is calculated based on the value in 'admitted' column. Load the example data.\n",
    "#\n",
    "load_example_data(\"dataframe\", \"admissions_train\")\n",
    "df = DataFrame('admissions_train')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map_row(): Example 1\n",
    "\n",
    "Create the user defined function to increase the 'gpa' by the percentage provided. Note that the input to and the output from the function is a pandas Series object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_gpa(row, p=20):\n",
    "    row['gpa'] = row['gpa'] + row['gpa'] * p/100\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the user defined function to the DataFrame.\n",
    "# Note that since the output of the user defined function expects the same\n",
    "# columns with the same types, we can skip passing the 'returns' argument.\n",
    "increase_gpa_20 = df.map_row(increase_gpa)\n",
    "\n",
    "# Print the result.\n",
    "increase_gpa_20.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map_row(): Example 2\n",
    "\n",
    "Use the same user defined function with a lambda notation to pass the percentage 'p = 40'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increase_gpa_40 = df.map_row(lambda row: increase_gpa(row, p = 40))\n",
    "\n",
    "increase_gpa_40.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map_row(): Example 3\n",
    "\n",
    "Use the same user defined function with functools.partial to pass the percentage 'p = 50'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "increase_gpa_50 = df.map_row(partial(increase_gpa, p = 50))\n",
    "\n",
    "increase_gpa_50.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map_row(): Example 4\n",
    "\n",
    "Use a lambda function to increase the 'gpa' by 50 percent, and return numpy ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "increase_gpa_lambda = lambda row, p=20: asarray([row['id'], row['masters'], row['gpa'] + row['gpa'] * p/100,\n",
    "                                                 row['stats'], row['programming'], row['admitted']])\n",
    "\n",
    "increase_gpa_100 = df.map_row(lambda row: increase_gpa_lambda(row, p=100))\n",
    "\n",
    "increase_gpa_100.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map_row(): Example 5\n",
    "\n",
    "Using non-default chunk_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using chunk_size = 5\n",
    "out_df = df.map_row(increase_gpa, chunk_size=5)\n",
    "\n",
    "out_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. map_partition(): Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example data and create the input DataFrames.\n",
    "#\n",
    "print(\"Loading data\")\n",
    "load_example_data(\"GLMPredict\", [\"housing_test\",\"housing_train\"])\n",
    "\n",
    "print(\"Creating dataframes\")\n",
    "train = DataFrame('housing_train')\n",
    "test = DataFrame('housing_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a preview of the DataFrames\n",
    "#\n",
    "print(\"Train dataset\")\n",
    "display(train.to_pandas().head(5))\n",
    "print(\"\\n\")\n",
    "print(\"Test dataset\")\n",
    "display(test.to_pandas().head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map_partition(): Example 1\n",
    "\n",
    "Model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that we want to use to fit multiple GLM models, one for each home style.\n",
    "# We will be using the statsmodels package here.\n",
    "\n",
    "def glm_fit(rows):\n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "        Function that accepts an iterator on a pandas DataFrame (TextFileObject) created using\n",
    "        'chunk_size' with pandas.read_csv(), and fits a GLM model to the corresponding data.\n",
    "        The underlying data is the housing data with 12 independent variable (inluding the home style)\n",
    "        and one dependent variable (price).\n",
    "    \n",
    "    RETURNS:\n",
    "        A numpy.ndarray object with two elements:\n",
    "        * The homestyle value (type: str)\n",
    "        * The GLM model that was fit to the corresponding data, which is serialized using pickle\n",
    "          and base64 encoded. We use decode() to make sure it is of type str, and not bytes.\n",
    "    \"\"\"\n",
    "    # Read the entire partition/group of rows in a pandas DataFrame - pdf.\n",
    "    data = rows.read()\n",
    "\n",
    "    # Add the 'intercept' column along with the features.\n",
    "    data['intercept'] = 1.0\n",
    "\n",
    "    # We will not process the partition if there are no rows here.\n",
    "    if data.shape[0] > 0:\n",
    "        # Fit the model using R-style formula to specify categorical variables as well.\n",
    "        # We use 'disp=0' to prevent sterr output.\n",
    "        model = smf.glm('price ~ C(recroom) + lotsize + stories + garagepl + C(gashw) +'\n",
    "                        ' bedrooms + C(driveway) + C(airco) + C(homestyle) + bathrms +'\n",
    "                        ' C(fullbase) + C(prefarea)',\n",
    "                        family=sm.families.Gaussian(), data=data).fit(disp=0)\n",
    "\n",
    "        # We serialize and base64 encode the model in prepration to output it.\n",
    "        modelSer = b64encode(dumps(model))\n",
    "        \n",
    "        # The user function can either return a value of supported type\n",
    "        # (numpy array, pandas Series, or pandas DataFrame),\n",
    "        # or just print it to find it's way to the output.\n",
    "        # Here we return it as a numpy ndarray object.\n",
    "        \n",
    "        # Note that we use decode for the serialized model so that it is\n",
    "        # represented in the ascii form (which is what base64 encoding does),\n",
    "        # instead of bytes.\n",
    "        return asarray([data.loc[0]['homestyle'], modelSer.decode('ascii')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the 'glm_fit' function defined above to create a model for every homestyle in\n",
    "# the training dataset.\n",
    "print(\"Fitting the model to the training data...\")\n",
    "\n",
    "# We specify the output column names and their types here with the 'returns'\n",
    "# argument since the output is not similar to the input.\n",
    "model = train.map_partition(glm_fit, data_partition_column = 'homestyle',\n",
    "                            returns = OrderedDict([('homestyle', train.homestyle.type),\n",
    "                                                   ('model', CLOB())]))\n",
    "\n",
    "# The model table has been created successfully.\n",
    "print(\"Model table has been created!\")\n",
    "display(model.to_pandas().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map_partition(): Example 2\n",
    "\n",
    "In this example, we score observations on the basis of the models trained in Example 1 above.\n",
    "\n",
    "We use window function 'row_number()' to assign row numbers to each subset of data corresponding to a particular homestyle. The idea is to extend the table to add the model corresponding to the homestyle as the last column value for the first row in the partition. This makes it easier for the scoring function to read the model and then score the input records based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create row number column ('row_id') in the 'test' DataFrame.\n",
    "test_with_row_num = test.assign(row_id = func.row_number().over(partition_by=test.homestyle.expression, order_by=test.sn.expression.desc()))\n",
    "\n",
    "# Join it with the model we created based on the value of homestyle.\n",
    "temp = test_with_row_num.join(model, on = [(test_with_row_num.homestyle == model.homestyle)], rsuffix='r', lsuffix='l')\n",
    "\n",
    "# Set the model column to NULL when row_id is not 1.\n",
    "temp = temp.assign(modeldata = case([(temp.row_id == 1, literal_column(temp.model.name))], else_ = None))\n",
    "\n",
    "# Drop the extraneous columns created in the processing.\n",
    "temp = temp.assign(homestyle = temp.l_homestyle).drop('l_homestyle', axis=1).drop('r_homestyle',axis=1).drop('model', axis=1)\n",
    "\n",
    "# Reorder the columns to have the housing data columns positioned first, followed by the row_id and modeldata.\n",
    "new_test = temp.select(test.columns + ['row_id', 'modeldata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIMITER = '\\t'\n",
    "QUOTECHAR = None\n",
    "\n",
    "def glm_score(rows):\n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "        Function that accepts an iterator on a pandas DataFrame (TextFileObject) created using\n",
    "        'chunk_size' with pandas.read_csv(), and scores it based on the model found in the data.\n",
    "        The undelrying data is the housing data with 12 independent variable (inluding the home style)\n",
    "        and one dependent variable (price).\n",
    "        \n",
    "        The function outputs the values itself, rather than returning objects of supported type.\n",
    "    \n",
    "    RETURNS:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    model = None\n",
    "    for chunk in rows:\n",
    "        # We process data only if there is any, i.e. only when the chunk read has any rows.\n",
    "        if chunk.shape[0] > 0:\n",
    "            if model is None:\n",
    "                # We read the model once (it is found only once) per partition.\n",
    "                model = loads(b64decode(chunk.loc[0].iloc[-1]))\n",
    "\n",
    "            # Exclude the row_id and modeldata columns from the scoring dataset as they are not longer required.\n",
    "            chunk = chunk.iloc[:,:-2]\n",
    "            # For prediction, exclude the first two columns ('sn' - not relevant, and 'price' - the dependent variable).\n",
    "            prediction = model.predict(chunk.iloc[:,2:])\n",
    "            \n",
    "            # We now concat the chunk with the prediction column (pandas Series) to form a DataFrame.\n",
    "            outdf = concat([chunk, prediction], axis=1)\n",
    "                            \n",
    "            # We just cannot return this DataFrame yet as we have more chunks to process.\n",
    "            # In such scenarios, we can either:\n",
    "            #   1. print the output here, or\n",
    "            #   2. keep concatenating the results of each chunk to create a final resultant pandas DataFrame to return.\n",
    "            # We are opting for option #1 here.\n",
    "            for _, row in outdf.iterrows():\n",
    "                if QUOTECHAR is not None:\n",
    "                    # A NULL value should not be enclosed in quotes.\n",
    "                    # The CSV module has no support for such output with writer, and hence the custom formatting.\n",
    "                    values = ['' if isna(s) else \"{}{}{}\".format(QUOTECHAR,\n",
    "                                                                 str(s), QUOTECHAR) for s in row]\n",
    "                else:\n",
    "                    values = ['' if isna(s) else str(s) for s in row]\n",
    "                print(DELIMITER.join(values), file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying glm_score function to the test data...\")\n",
    "# Note that here the output of the function is going to have one more column than the input,\n",
    "# and we must specify the same.\n",
    "returns = OrderedDict([(col.name, col.type) for col in test._metaexpr.c] + [('prediction', FLOAT())])\n",
    "\n",
    "# Note that we are using the 'data_order_column' argument here to order by the 'row_id'\n",
    "# column so that the model is read before any data that need to be scored.\n",
    "prediction = new_test.map_partition(glm_score,\n",
    "                                    returns = returns,\n",
    "                                    data_partition_column = 'homestyle',\n",
    "                                    data_order_column = 'row_id')\n",
    "print(\"Scoring complete!\")\n",
    "\n",
    "# Print a sample of the scoring result.\n",
    "prediction.to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, remove_context() to mark the end of the session and drop all temporary objects created.\n",
    "remove_context()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
